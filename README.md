# Conversion of Audioless Video to Speech Using AV-HuBERT Algorithm

## Overview
This project reconstructs speech from lip movements using deep learning techniques. The model processes uploaded video files, extracts visual features, and generates corresponding speech output.

## Features
- Uses deep learning for lip-reading and speech synthesis
- Converts lip movements to text and speech
- Streamlit-based web interface
- Supports MP4 video format for input

## Installation

### Prerequisites
Ensure you have the following installed:
- Python 3.8+
- pip
- Virtual environment (optional but recommended)

### Setup
```bash
# Clone the repository
git clone https://github.com/yourusername/lip-reading-project.git
cd lip-reading-project

# Install dependencies
pip install -r requirements.txt
```

## Usage
Run the Streamlit web application:
```bash
streamlit run lip_reading_project.py
```

## File Structure
```
├── AV-HuBERT.py  # Main application script
├── requirements.txt  # Required dependencies
├── README.md  # Project documentation
```

## Publication
https://doi.org/10.1007/978-3-031-69201-7_32

## Contributing
If you wish to contribute, please fork the repository and submit a pull request with a detailed explanation of your changes.

## License
This project is licensed under the MIT License - see the LICENSE file for details.

